from __future__ import annotations

import subprocess
from dataclasses import dataclass
from pathlib import Path
from tempfile import TemporaryDirectory

from packages.config.gpu_detector import get_whisper_device


@dataclass(slots=True)
class WordTimestamp:
    word: str
    start: float
    end: float
    probability: float


class Transcriber:
    """
    Faster-Whisper wrapper with optional chunked transcription.
    """

    def __init__(
        self,
        *,
        model_name: str = "small",
        device: str = "auto",
        chunk_duration_sec: int = 1800,
    ) -> None:
        self.model_name = model_name
        self.device = get_whisper_device() if device == "auto" else device
        self.chunk_duration_sec = chunk_duration_sec
        self._model = None

    def transcribe(self, audio_path: str | Path) -> list[WordTimestamp]:
        source = Path(audio_path)
        duration = self._probe_duration(source)
        if duration is None or duration <= self.chunk_duration_sec:
            return self._transcribe_single(source, offset_sec=0.0)

        all_words: list[WordTimestamp] = []
        with TemporaryDirectory(prefix="autoclipper-chunks-") as temp_dir:
            chunks = self._split_audio(source, Path(temp_dir))
            for chunk_file, offset in chunks:
                all_words.extend(self._transcribe_single(chunk_file, offset_sec=offset))
        return all_words

    def _transcribe_single(self, audio_path: Path, *, offset_sec: float) -> list[WordTimestamp]:
        model = self._load_model()
        segments, _info = model.transcribe(
            str(audio_path),
            word_timestamps=True,
            vad_filter=True,
            beam_size=5,
        )
        words: list[WordTimestamp] = []
        for segment in segments:
            segment_words = getattr(segment, "words", None) or []
            for word in segment_words:
                token = str(getattr(word, "word", "")).strip()
                if not token:
                    continue
                words.append(
                    WordTimestamp(
                        word=token,
                        start=float(getattr(word, "start", 0.0)) + offset_sec,
                        end=float(getattr(word, "end", 0.0)) + offset_sec,
                        probability=float(getattr(word, "probability", 0.0)),
                    )
                )
        return words

    def _load_model(self):
        if self._model is not None:
            return self._model
        try:
            from faster_whisper import WhisperModel  # type: ignore
        except Exception as exc:
            raise RuntimeError("faster-whisper is not installed") from exc
        compute_type = "float16" if self.device == "cuda" else "int8"
        self._model = WhisperModel(self.model_name, device=self.device, compute_type=compute_type)
        return self._model

    def _probe_duration(self, audio_path: Path) -> float | None:
        command = [
            "ffprobe",
            "-v",
            "error",
            "-show_entries",
            "format=duration",
            "-of",
            "default=noprint_wrappers=1:nokey=1",
            str(audio_path),
        ]
        try:
            result = subprocess.run(command, capture_output=True, text=True, check=False)
            if result.returncode != 0:
                return None
            return float(result.stdout.strip())
        except Exception:
            return None

    def _split_audio(self, source: Path, temp_dir: Path) -> list[tuple[Path, float]]:
        output_pattern = temp_dir / "chunk_%04d.wav"
        command = [
            "ffmpeg",
            "-y",
            "-i",
            str(source),
            "-f",
            "segment",
            "-segment_time",
            str(self.chunk_duration_sec),
            "-c",
            "copy",
            str(output_pattern),
        ]
        result = subprocess.run(command, capture_output=True, text=True, check=False)
        if result.returncode != 0:
            raise RuntimeError(f"Failed splitting audio: {result.stderr.strip()}")

        chunks = sorted(temp_dir.glob("chunk_*.wav"))
        if not chunks:
            raise RuntimeError("No chunks generated by ffmpeg segmenter")
        mapped: list[tuple[Path, float]] = []
        for index, chunk in enumerate(chunks):
            mapped.append((chunk, float(index * self.chunk_duration_sec)))
        return mapped
